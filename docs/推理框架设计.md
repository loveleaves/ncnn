# 推理框架设计
##  框架总体架构
一般神经网络推理框架包含以下几个部分：
- 输入层：接收输入数据并进行预处理。
- 推理引擎：加载模型，执行前向传播过程，计算神经网络的输出。
- 输出层：生成推理结果并进行后处理。
- 硬件加速：支持 GPU、CPU 或其他硬件（如 FPGA、TPU）进行推理加速。
- 优化策略：包括量化、剪枝、知识蒸馏等优化方法以提高推理效率。
- API层：提供与外部系统交互的接口，支持多种框架和模型的加载。

ncnn框架的推理框架设计策略：

## 输入层
- 读取：openCV读入或minimal opencv读入
- 预处理：双线插值等图像预处理
- 输入策略：主要针对单输入，全部读取并处理
- 编程语言策略：数据及操作全部置于Mat类中，用C++特性管理

## 推理引擎
推理引擎主要包含两部分，加载计算图及模型参数和执行前向传播。
- 计算图一般包含基本数据结构张量（Tensor）和基本运算单元算子，一般用有向无环图中的边和节点表示。在ncnn中，节点为layer，数据节点（单输入边多输出边，为layer的输出）为blob。
- 计算图结构保存在param中（可用ncnn2mem加密），参数保存在bin中
- 前向推理采用策略：懒惰推理（Lazy Inference） 或 懒加载（Lazy Evaluation），大流量场景考虑批量推理（Batch Inference）、异步推理（Asynchronous Inference）、缓存机制（Caching）等策略
- 编程语言策略：具体算子继承抽象类layer，各自保存权重参数；整个网络net类管理，并由友元类extractor喂入网络输入和调用前向传播。

## 输出层
- 编程语言策略：传入net友元类extractor前向推理获取输出结果

## 硬件加速
TODO

## 优化策略
- fp16、int8量化

## API层
- 数据处理：Mat类
- 模型读取：net类
- 模型推理：extractor类